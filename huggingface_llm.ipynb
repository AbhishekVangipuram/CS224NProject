{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://huggingface.co/docs/transformers/v4.41.0/en/llm_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1b293ff04f4db891bc99871000865a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"'The soup is hot' translated to the Southern Nigerian language Obolo is 'Egbele'. This phrase is used to warn people of the danger of taking a hot meal, especially when it is served in a communal setting. The phrase is also used to caution people against getting involved in a situation that may be harmful or dangerous. It is a way of saying 'be careful' or 'watch out'. In Obolo, 'Egbele' is a warning that something is hot and may cause harm, and it is used to protect people from potential danger\",\n",
       " \"'The soup is hot' translated to the Southern Indian language Tamil is 'Kootu varum'. In this context, 'Kootu' means soup and 'varum' means is hot.\\nIn the Southern Indian language Telugu, 'The soup is hot' is translated to 'Kootu pedata'. Here, 'Kootu' means soup and 'pedata' means is hot.\\nIn the Southern Indian language Malayalam, 'The soup is hot' is translated to 'Kootu kaanum'. Here, 'Kootu\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"'The soup is hot' translated to the Southern Nigerian language Obolo is\", \"'The soup is hot' translated to the Southern Indian language Tamil is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['End your answer to the following question with the tag \\'[END]\\' and do not provide anything but the answer to the question. What is the French translation of \\'The small fox loved croissants.\\'? [BEGIN] The French translation of \\'The small fox loved croissants.\\' is \\'Le petit renard aimait les croissants.\\' [END]... Read more →\\nPosted at 11:30 AM in French, Language Translation | Permalink | Comments (0)\\nWhat is the French translation of \"The small fox loved croissants.\"?\\n[BEGIN]\\nThe French translation of \"The small fox loved croissants.\" is \"Le petit renard aimait les croissants.\"\\n[',\n",
       " \"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the Obolo translation of 'The small bird loved grass.'? [BEGIN] The Obolo translation of 'The small bird loved grass.' is 'Ibibi ebelebi ebele.' [END]...\\n\\n### Other questions from the same topic\\n\\nWhat is the Obolo translation of 'The small bird loved grass.'? [BEGIN] The Obolo translation of 'The small bird loved grass.' is 'Ibibi ebelebi ebele.' [END]...\\n\\nWhat is the Obolo translation of 'The small bird loved grass.'? [\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the French translation of 'The small fox loved croissants.'? [BEGIN]\",\n",
    "     \"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9bd1aa1e1e4f939f9405b747286edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded output:\n",
      " system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]assistant\n",
      "\n",
      "Le renard petit aimait les croissants.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input as before\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the French translation of 'The small fox loved croissants.'? [BEGIN]\"}\n",
    "]\n",
    "\n",
    "# 1: Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, load_in_8bit=True, attn_implementation=\"sdpa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "\n",
    "# 2: Apply the chat template\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(\"Formatted chat:\\n\", formatted_chat)\n",
    "\n",
    "# 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "# Move the tokenized inputs to the same device the model is on (GPU/CPU)\n",
    "# inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "# print(\"Tokenized inputs:\\n\", inputs)\n",
    "\n",
    "# 4: Generate text from the model\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "# print(\"Generated tokens:\\n\", generated_ids)\n",
    "\n",
    "# 5: Decode the output back to a string\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded output:\\n\", decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]assistant\n",
      "\n",
      "Ibo: Nkpo mkpo na-akpa ọkụ. [END]\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]assistant\n",
      "\n",
      "Abi, abasi, abakwa, abana, abana. [END]\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Tamil translation of 'The man is young.'? [BEGIN]assistant\n",
      "\n",
      "ஆண் இளையவர்.\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Tamil translation of 'The man is young.'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do evaluation on of the model on our Bible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Verse', 'Obolo', 'English'],\n",
       "        num_rows: 31097\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(examples):\n",
    "#     return tokenizer(examples[\"Obolo\"], examples[\"English\"], padding=\"max_length\")\n",
    "\n",
    "# dataset = dataset.map(encode, batched=True)\n",
    "# # dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Verse', 'Obolo', 'English'],\n",
       "        num_rows: 27987\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Verse', 'Obolo', 'English'],\n",
       "        num_rows: 3110\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_splits = dataset['train'].train_test_split(0.1)\n",
    "data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Verse', 'Obolo', 'English'],\n",
       "    num_rows: 27987\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Verse', 'Obolo', 'English'],\n",
       "    num_rows: 3110\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = data_splits['train'], data_splits['test']\n",
    "display(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load, combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abhiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "chrf = load('chrf')\n",
    "gleu = load('google_bleu')\n",
    "rouge = load('rouge') \n",
    "bleu = load('bleu')\n",
    "meteor = load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = combine([chrf, bleu, rouge, meteor, gleu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now do a check that these metrics work on a default dataset\n",
    "rt_data = load_dataset(\"rotten_tomatoes\")\n",
    "rt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3cc1428be04c5bbdfe1063af6dd3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f90b3022f6e4c619d4d28b2fa0cd27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba6da5b222b4007aff12f91a3136b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def tokenization(example):\n",
    "#     return tokenizer(example[\"text\"])\n",
    "\n",
    "# rt_data = rt_data.map(tokenization, batched=True)\n",
    "# # rt_data.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
    "# rt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sentiment analysis bot. Given an input sentence, output 0 if it has negative sentiment and 1 if it has positive sentiment. Your answer is always exactly 1 character long.\"}\n",
    "]\n",
    "\n",
    "length = 10\n",
    "preds = []\n",
    "for idx, line in enumerate(rt_data['test']['text'][:length]):\n",
    "    chat += [{\"role\": \"user\", \"content\": line} ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    preds.append(decoded_output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "preds = [int(p) for p in preds]\n",
    "print(preds)\n",
    "refs = rt_data['test']['label'][:length]\n",
    "print(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'f1': 0.888888888888889}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score = metric.compute(predictions=preds, references=refs)\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The people of Awaji are being killed by the war, but they are not being defeated; the war is being fought, and we are being killed in it.', 'The elders are saying that they will not be happy if we do not bring back our brothers who went to the war; and they are saying that if we do not bring them back, the people of the land will not forgive us.']\n",
      "['behold, happy [is] the man whom god correcteth: therefore despise not thou the chastening of the almighty:', 'so the priests and the prophets and all the people heard jeremiah speaking these words in the house of the lord.']\n"
     ]
    }
   ],
   "source": [
    "beg_tok, end_tok = '[BEGIN]', '[END]'\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are an expert translator in Obolo and English. You will simply translate the given line from Obolo into English. Your answer will start with a {beg_tok} tag and end with a {end_tok} tag. Do not repeat the question or provide any other text that is not the translation of the provided text.\"}\n",
    "]\n",
    "\n",
    "length = 5\n",
    "preds = []\n",
    "for idx, line in enumerate(test['Obolo'][:length]):\n",
    "    chat += [{\"role\": \"user\", \"content\": line} ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=512, do_sample=True)\n",
    "    decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    idx_beg, idx_end = decoded_output.rindex(beg_tok), decoded_output.rindex(end_tok)\n",
    "    preds.append(decoded_output[idx_beg+len(beg_tok):idx_end].strip())\n",
    "\n",
    "refs  = test['English'][:length]\n",
    "\n",
    "print(preds[:2])\n",
    "print(refs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.631105127578163,\n",
       " 'char_order': 6,\n",
       " 'word_order': 0,\n",
       " 'beta': 2,\n",
       " 'bleu': 0.0,\n",
       " 'precisions': [0.10869565217391304, 0.01675977653631285, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.936842105263158,\n",
       " 'translation_length': 184,\n",
       " 'reference_length': 95,\n",
       " 'rouge1': 0.11044534412955465,\n",
       " 'rouge2': 0.012698412698412698,\n",
       " 'rougeL': 0.083903990746096,\n",
       " 'rougeLsum': 0.08,\n",
       " 'meteor': 0.12263219123316241,\n",
       " 'google_bleu': 0.032577903682719546}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = metrics.compute(predictions=preds, references=refs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5faea850a08454385161ee1bb432b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"google/gemma-1.1-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, load_in_8bit=True, attn_implementation=\"sdpa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 17.093869231335184,\n",
       " 'char_order': 6,\n",
       " 'word_order': 0,\n",
       " 'beta': 2,\n",
       " 'bleu': 0.0,\n",
       " 'precisions': [0.08349146110056926, 0.0038684719535783366, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.271551724137931,\n",
       " 'translation_length': 527,\n",
       " 'reference_length': 232,\n",
       " 'rouge1': 0.07797474302118265,\n",
       " 'rouge2': 0.000790513833992095,\n",
       " 'rougeL': 0.07364187240348231,\n",
       " 'rougeLsum': 0.0740951817499805,\n",
       " 'meteor': 0.08617908313991111,\n",
       " 'google_bleu': 0.021904761904761906}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beg_tok, end_tok = '[BEGIN]', '[END]'\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are an expert translator in Obolo and English. You will simply translate the given line from Obolo into English. Your answer will start with a {beg_tok} tag and end with a {end_tok} tag. Do not repeat the question or provide any other text that is not the translation of the provided text.\"}\n",
    "]\n",
    "\n",
    "length = 10\n",
    "preds = []\n",
    "for idx, line in enumerate(test['Obolo'][:length]):\n",
    "    # chat += [{\"role\": (\"user\" if idx%2 else \"assistant\"), \"content\": line} ]\n",
    "    chat += [{\"role\": \"user\", \"content\": line} ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=512, do_sample=True)\n",
    "    decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    idx_beg, idx_end = decoded_output.rindex(beg_tok), decoded_output.rindex(end_tok)\n",
    "    preds.append(decoded_output[idx_beg+len(beg_tok):idx_end].strip())\n",
    "\n",
    "refs  = test['English'][:length]\n",
    "\n",
    "# print(preds[:2])\n",
    "# print(refs[:2])\n",
    "scores = metrics.compute(predictions=preds, references=refs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"My mother's friend's child is the one who has eaten the yam with the man's child.\",\n",
       "  'and over the host of the tribe of the children of issachar [was] nethaneel the son of zuar.'),\n",
       " ('Jonathan Ofofook says, \"I\\'ll take care of it! The burden of Solomon\\'s debt is on me.\"',\n",
       "  'and jonathan answered and said to adonijah, verily our lord king david hath made solomon king.'),\n",
       " ('Jonathan said, \"What a shame! The king\\'s daughter is in debt, and she is asking for Solomon\\'s help.\"',\n",
       "  'mattaniah, mattenai, and jaasau,'),\n",
       " ('There, I have taken the fruit, I have taken the fruit of the land, I have taken the fruit of the land and given to the king, I have given to the king and his son. There, I have taken the fruit, I have taken the fruit of the land.',\n",
       "  'but the days will come, when the bridegroom shall be taken away from them, and then shall they fast in those days.'),\n",
       " ('“The children are playing, they are happy, but the adults are not at home, they are going to the farm, and I will go to the village, I will not come back until tomorrow.”',\n",
       "  'saying, hurt not the earth, neither the sea, nor the trees, till we have sealed the servants of our god in their foreheads.'),\n",
       " ('“the elder will not accept the gift of a child, but the elder will accept the gift of a child after the child has grown up. the elder will not accept the gift of a child because the child is still young, but the elder will accept the gift of a child when the child has matured. the elder will accept the gift of a child when the child has become useful.”',\n",
       "  'that which was torn [of beasts] i brought not unto thee; i bare the loss of it; of my hand didst thou require it, [whether] stolen by day, or stolen by night.'),\n",
       " ('tag and end with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.assistant\\n\\nnetanen gwun̄ kè juwari ore ogwu ibot ebi otoko kè isaka.user\\n\\njonatan ofọfọọk ibe, “ìkare kubọk ge! ogwukaan̄ kiji ogwu ubọọn̄ debit îrọ solomọn ubọọn̄.assistant\\n\\nmataniya, mè matenai, mè jayasu.user\\n\\nire, mgbọ òkinu, mgbọ mêsan̄abe ogwu òdo nwa isan̄a me etete kiban̄. îre mgbọ keya ke ema ebejin otu.”assistant\\n\\n“ikajijaka-nu ijọn̄ mè okwaan̄, mè uti geege, inyi eji edasi etap iman̄ òriọọn̄ me inyọn̄ okwut isi ebi ikwaan̄ awaji kiji.”user\\n\\nmgbọ geelek unye anam okpan̄be anam kwun̄, ǹkisasa anam eyi n̄a itap ichit, ǹkasa nkwukwu anam ìjet owu. mgbọ geelek inọ ochiin̄be anam kwun̄, mè ìre ikarek, mè ìre eririeen̄, owu òkitetet emi ibọkọ useen̄.assistant\\n\\n“ebi ichen ìluk me ama ya ekibe, ‘eji kè ègwa babilọn, ire, kpebekọt igwa ọmọ; kè eji ejit ọmọ etele, mè ogwu ikup, ifo ido kiban̄, mije, oyerebet kan̄ îre akpabin inyọn̄, mè ibene ire lek oduku inyọn̄.’assistant',\n",
       "  'we would have healed babylon, but she is not healed: forsake her, and let us go every one into his own country: for her judgment reacheth unto heaven, and is lifted up [even] to the skies.'),\n",
       " ('tag and end with a', 'o earth, earth, earth, hear the word of the lord.'),\n",
       " ('Oh, my father, my father, my father, how I wish I could see you!',\n",
       "  'to do justice and judgment [is] more acceptable to the lord than sacrifice.'),\n",
       " ('\"the old man, with his staff, went to the forest, to hunt for food, and he said, \\'if I do not return, my children will be orphans, and my wife will be a widow, and my people will be in sorrow.\\'',\n",
       "  'but them that are without god judgeth. therefore put away from among yourselves that wicked person.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(preds, refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 17.093869231335184,\n",
       " 'char_order': 6,\n",
       " 'word_order': 0,\n",
       " 'beta': 2,\n",
       " 'bleu': 0.0,\n",
       " 'precisions': [0.08349146110056926, 0.0038684719535783366, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.271551724137931,\n",
       " 'translation_length': 527,\n",
       " 'reference_length': 232,\n",
       " 'rouge1': 0.07797474302118265,\n",
       " 'rouge2': 0.000790513833992095,\n",
       " 'rougeL': 0.07364187240348231,\n",
       " 'rougeLsum': 0.0740951817499805,\n",
       " 'meteor': 0.08617908313991111,\n",
       " 'google_bleu': 0.021904761904761906}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse244",
   "language": "python",
   "name": "mse244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
