{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://huggingface.co/docs/transformers/v4.41.0/en/llm_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2829f28a944fb7a54fee12cfb2b3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"'The soup is hot' translated to the Southern Nigerian language Obolo is 'Owo-eele'. This phrase is used to warn people of the danger of a hot soup or food. It is a common phrase used in the Southern Nigerian language Obolo, which is spoken by the Obolo people of Akwa Ibom State, Nigeria. The phrase is used to caution people not to touch or eat the hot food without being careful, as it can cause burns or other harm. It is a phrase that is deeply ingrained in the culture and language of the Ob\",\n",
       " \"'The soup is hot' translated to the Southern Indian language Tamil is 'Kuzha vendum'. So, when you say 'Kuzha vendum' to someone, you're essentially saying 'The soup is hot'! Isn't that a fun fact? 😊\\n#TamilLanguage #SouthernIndian #Food #CulturalFacts #FunFacts #LanguageLovers #Foodie #CulturalExchange #LanguageExchange #Multilingual #LanguageLearning #FoodCulture #CulturalHeritage #TamilCulture #IndianCulture #LanguageAndCulture\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"'The soup is hot' translated to the Southern Nigerian language Obolo is\", \"'The soup is hot' translated to the Southern Indian language Tamil is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['End your answer to the following question with the tag \\'[END]\\' and do not provide anything but the answer to the question. What is the French translation of \\'The small fox loved croissants.\\'? [BEGIN] The French translation of \\'The small fox loved croissants.\\' is \\'Le petit renard aimait les croissants.\\' [END]... Read more →\\nPosted at 11:30 AM in French, Language Translation | Permalink | Comments (0)\\nWhat is the French translation of \"The small fox loved croissants.\"?\\n[BEGIN]\\nThe French translation of \"The small fox loved croissants.\" is \"Le petit renard aimait les croissants.\"\\n[',\n",
       " \"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the Obolo translation of 'The small bird loved grass.'? [BEGIN] The Obolo translation of 'The small bird loved grass.' is 'Ibibi ebelebi ebele.' [END]...\\n\\n### Other questions from the same topic\\n\\nWhat is the Obolo translation of 'The small bird loved grass.'? [BEGIN] The Obolo translation of 'The small bird loved grass.' is 'Ibibi ebelebi ebele.' [END]...\\n\\nWhat is the Obolo translation of 'The small bird loved grass.'? [\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the French translation of 'The small fox loved croissants.'? [BEGIN]\",\n",
    "     \"End your answer to the following question with the tag '[END]' and do not provide anything but the answer to the question. What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9bd1aa1e1e4f939f9405b747286edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded output:\n",
      " system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]assistant\n",
      "\n",
      "Le renard petit aimait les croissants.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input as before\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the French translation of 'The small fox loved croissants.'? [BEGIN]\"}\n",
    "]\n",
    "\n",
    "# 1: Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, load_in_8bit=True, attn_implementation=\"sdpa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "\n",
    "# 2: Apply the chat template\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(\"Formatted chat:\\n\", formatted_chat)\n",
    "\n",
    "# 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "# Move the tokenized inputs to the same device the model is on (GPU/CPU)\n",
    "# inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "# print(\"Tokenized inputs:\\n\", inputs)\n",
    "\n",
    "# 4: Generate text from the model\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "# print(\"Generated tokens:\\n\", generated_ids)\n",
    "\n",
    "# 5: Decode the output back to a string\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded output:\\n\", decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]assistant\n",
      "\n",
      "Ibo: Nkpo mkpo na-akpa ọkụ. [END]\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]assistant\n",
      "\n",
      "Abi, abasi, abakwa, abana, abana. [END]\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You are an expert translator in many languages. You will simply answer the given translation question, which starts with a [BEGIN] tag and ends with a [END] tag. Do not repeat the question or provide any other text that is not the translation of the provided text.user\n",
      "\n",
      "What is the French translation of 'The small fox loved croissants.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'The small bird loved grass.'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Obolo translation of 'one, two, three, four, five'? [BEGIN]user\n",
      "\n",
      "What is the Tamil translation of 'The man is young.'? [BEGIN]assistant\n",
      "\n",
      "ஆண் இளையவர்.\n"
     ]
    }
   ],
   "source": [
    "chat.append(\n",
    "    {\"role\": \"user\", \"content\": \"What is the Tamil translation of 'The man is young.'? [BEGIN]\"}\n",
    ")\n",
    "\n",
    "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(\"cuda\")\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do evaluation on of the model on our Bible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "chrf = evaluate.load('chrf')\n",
    "gleu = evaluate.load('google_bleu')\n",
    "rouge = evaluate.load('rouge') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse244",
   "language": "python",
   "name": "mse244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
