{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "# model_id = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model_id = \"google-t5/t5-base\"\n",
    "\n",
    "# does not download the pretrained weights, just affects configuration\n",
    "# use AutoModelForSeq2SeqLM.from_pretrained to also download the weights\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now, need to add a tokenizer and embedding layer to the top of the model\n",
    "auto_tk = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "['”', '▁punct', '▁taken', 'elecommunication', '▁cables', '▁helpful', 'cita', 'gasesc', '▁buyer', 'aide', '▁bisher', '▁upgrading', '▁Haftung', '▁crunchy', '▁Colegi', '▁Bollywood', '▁historical', '▁NASA', '▁Minute', 'teamed', '▁peace', '▁Diesel', '▁--', 'gate', '▁zip', '▁zuständig', '▁define', '▁Digi', '▁Diversity', '▁engage', '▁peninsula', 'moni', '▁phone', '▁reflecting', '▁experienta', 'blin', '▁poems', 'zugleich', '▁force', 'brücke', 'Lib', 'cul', 'ministerium', '▁song', '▁themes', '▁suis', 'admi', 'gesagt', 'â', 'OC', '▁birouri', '▁activités', '▁franchi', '▁Cushion', '▁Versand', '▁mittels', '▁strig', '▁diffusion', 'lebt', '▁payment', '▁crashes', '▁Qualcomm', '▁Strange', 'lov', '▁nächste', '▁Januar', '▁bestellen', '▁Sat', '▁aplicat', '▁revolution', 'soluble', '▁legend', 'terribly', '▁fitted', '▁run', '▁everyone', '▁ramp', '▁Fotos', 'absorbed', '▁ignor', '▁Bangalore', '▁Commissioner', 'ani', '▁funnel', '▁Proceedings', 'erweise', 'tech', '▁GREAT', '▁dilemma', 'identifying', '▁(\"', 'opia', 'ărilor', '▁ferry', 'transforming', '▁vân', 'Nu', 'obtenir', 'geschrieben', '▁vision']\n"
     ]
    }
   ],
   "source": [
    "print(auto_tk.vocab_size)\n",
    "print(list(auto_tk.get_vocab())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "# data_splits = dataset['train'].train_test_split(0.1)\n",
    "# train, test = data_splits['train'], data_splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./data/train.csv')\n",
    "# test.to_csv('./data/test.csv')\n",
    "train = load_dataset('csv', data_files='data/train.csv')['train']\n",
    "test = load_dataset('csv', data_files='data/test.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "obolo_data = train['Obolo']\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train_from_iterator(obolo_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15866\n",
      "30535\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in obolo_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“ekene onene ifo emen inyọn̄, mè isibi inu? sà ìre ekene otitiin̄ efet itap me akajit ubọk kan̄ ichit? ekene okat mun̄ me emen ekwut ibọp? sà ìre ekene osun̄ ntutun̄ ere geelek me linyọn̄ me irek kan̄? keke ore erieen̄ kan̄? sà ìre keke ore erieen̄ gwun̄ kan̄? tumu nyi emi nu, mè ire oriọọn̄!\n",
      "['ĠâĢľ', 'ekene', 'Ġonene', 'Ġifo', 'Ġemen', 'Ġinyo', 'Ì£', 'n', 'ÌĦ,', 'Ġme', 'ÌĢ', 'Ġisibi', 'Ġinu', '?', 'Ġsa', 'ÌĢ', 'Ġi', 'ÌĢ', 're', 'Ġekene', 'Ġotitiin', 'ÌĦ', 'Ġefet', 'Ġitap', 'Ġme', 'Ġakajit', 'Ġubo', 'Ì£', 'k', 'Ġkan', 'ÌĦ', 'Ġichit', '?', 'Ġekene', 'Ġokat', 'Ġmun', 'ÌĦ', 'Ġme', 'Ġemen', 'Ġekwut', 'Ġibo', 'Ì£', 'p', '?', 'Ġsa', 'ÌĢ', 'Ġi', 'ÌĢ', 're', 'Ġekene', 'Ġosun', 'ÌĦ', 'Ġntutun', 'ÌĦ', 'Ġere', 'Ġgeelek', 'Ġme', 'Ġlinyo', 'Ì£', 'n', 'ÌĦ', 'Ġme', 'Ġirek', 'Ġkan', 'ÌĦ?', 'Ġkeke', 'Ġore', 'Ġerieen', 'ÌĦ', 'Ġkan', 'ÌĦ?', 'Ġsa', 'ÌĢ', 'Ġi', 'ÌĢ', 're', 'Ġkeke', 'Ġore', 'Ġerieen', 'ÌĦ', 'Ġgwun', 'ÌĦ', 'Ġkan', 'ÌĦ?', 'Ġtumu', 'Ġnyi', 'Ġemi', 'Ġnu', ',', 'Ġme', 'ÌĢ', 'Ġire', 'Ġorio', 'Ì£', 'o', 'Ì£', 'n', 'ÌĦ!']\n",
      "jerimaya onenitumu inyi ebi owot iman kè rekabu cha ibe: “okumugwem ogwukaan̄ inyọn̄ mè ijọn̄, awaji ebi ijeren, ìtumu ibe ikeyi: ‘mije enyi etetbe ikan nte nte kinyi, jonadabu, onyibe enyi ikaan̄ inye, mè igban̄ utọn̄ me lek otutuuk nteme kan̄, mè irọ inu geelek îtọbe,\n",
      "['Ġjerimaya', 'Ġonenitumu', 'Ġinyi', 'Ġebi', 'Ġowot', 'Ġiman', 'Ġke', 'ÌĢ', 'Ġrekabu', 'Ġcha', 'Ġibe', ':', 'ĠâĢľ', 'okumugwem', 'Ġogwukaan', 'ÌĦ', 'Ġinyo', 'Ì£', 'n', 'ÌĦ', 'Ġme', 'ÌĢ', 'Ġijo', 'Ì£', 'n', 'ÌĦ,', 'Ġawaji', 'Ġebi', 'Ġijeren', ',', 'Ġi', 'ÌĢ', 'tumu', 'Ġibe', 'Ġikeyi', ':', 'ĠâĢĺ', 'mije', 'Ġenyi', 'Ġetetbe', 'Ġikan', 'Ġnte', 'Ġnte', 'Ġkinyi', ',', 'Ġjonadabu', ',', 'Ġonyibe', 'Ġenyi', 'Ġikaan', 'ÌĦ', 'Ġinye', ',', 'Ġme', 'ÌĢ', 'Ġigban', 'ÌĦ', 'Ġuto', 'Ì£', 'n', 'ÌĦ', 'Ġme', 'Ġlek', 'Ġotutuuk', 'Ġnteme', 'Ġkan', 'ÌĦ,', 'Ġme', 'ÌĢ', 'Ġiro', 'Ì£', 'Ġinu', 'Ġgeelek', 'Ġi', 'ÌĤ', 'to', 'Ì£', 'be', ',']\n",
      "ìkakup me uyọt ibe emi ǹge ikpa ǹjet enyi ofolek ikwaan̄ ntap-ubọk eyi ekiria ijet ebi eyi awaji ìkup me judia.\n",
      "['Ġi', 'ÌĢ', 'kakup', 'Ġme', 'Ġuyo', 'Ì£', 't', 'Ġibe', 'Ġemi', 'Ġn', 'ÌĢ', 'ge', 'Ġikpa', 'Ġn', 'ÌĢ', 'jet', 'Ġenyi', 'Ġofolek', 'Ġikwaan', 'ÌĦ', 'Ġntap', '-', 'ubo', 'Ì£', 'k', 'Ġeyi', 'Ġekiria', 'Ġijet', 'Ġebi', 'Ġeyi', 'Ġawaji', 'Ġi', 'ÌĢ', 'kup', 'Ġme', 'Ġjudia', '.']\n",
      "ire ata etip eji ekilook ikitumu ibe ke awaji îjomo karais me mkpa, usini ebi kinyi ejeje ikitumu ibe ke ebi ìkwakwaan̄ kpebejomo me mkpa?\n",
      "['Ġire', 'Ġata', 'Ġetip', 'Ġeji', 'Ġekilook', 'Ġikitumu', 'Ġibe', 'Ġke', 'Ġawaji', 'Ġi', 'ÌĤ', 'jomo', 'Ġkarais', 'Ġme', 'Ġmkpa', ',', 'Ġusini', 'Ġebi', 'Ġkinyi', 'Ġejeje', 'Ġikitumu', 'Ġibe', 'Ġke', 'Ġebi', 'Ġi', 'ÌĢ', 'kwakwaan', 'ÌĦ', 'Ġkpebejomo', 'Ġme', 'Ġmkpa', '?']\n",
      "“ìtumu ìnyi ejikaya ogwu ubọọn̄ juda ìbe: kachieek ibe awaji kwun̄ ya ogwu owu otoon̄be ejit me lek ifiaan̄ owu ibe ke jeruselem ìkponin̄ me ubọk ogwu ubọọn̄ asiria!\n",
      "['ĠâĢľ', 'i', 'ÌĢ', 'tumu', 'Ġi', 'ÌĢ', 'nyi', 'Ġejikaya', 'Ġogwu', 'Ġubo', 'Ì£', 'o', 'Ì£', 'n', 'ÌĦ', 'Ġjuda', 'Ġi', 'ÌĢ', 'be', ':', 'Ġkachieek', 'Ġibe', 'Ġawaji', 'Ġkwun', 'ÌĦ', 'Ġya', 'Ġogwu', 'Ġowu', 'Ġotoon', 'ÌĦ', 'be', 'Ġejit', 'Ġme', 'Ġlek', 'Ġifiaan', 'ÌĦ', 'Ġowu', 'Ġibe', 'Ġke', 'Ġjeruselem', 'Ġi', 'ÌĢ', 'kponin', 'ÌĦ', 'Ġme', 'Ġubo', 'Ì£', 'k', 'Ġogwu', 'Ġubo', 'Ì£', 'o', 'Ì£', 'n', 'ÌĦ', 'Ġasiria', '!']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(obolo_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(obolo_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "english_data = train['English']\n",
    "\n",
    "eng_tokenizer.pre_tokenizer = ByteLevel()\n",
    "eng_tokenizer.decoder = decoders.ByteLevel()\n",
    "eng_tokenizer.train_from_iterator(english_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15852\n",
      "28190\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in english_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and he said unto him, we [are] passing from bethlehemjudah toward the side of mount ephraim; from thence [am] i: and i went to bethlehemjudah, but i [am now] going to the house of the lord; and there [is] no man that receiveth me to house.\n",
      "['Ġand', 'Ġhe', 'Ġsaid', 'Ġunto', 'Ġhim', ',', 'Ġwe', 'Ġ[', 'are', ']', 'Ġpassing', 'Ġfrom', 'Ġbethlehemjudah', 'Ġtoward', 'Ġthe', 'Ġside', 'Ġof', 'Ġmount', 'Ġephraim', ';', 'Ġfrom', 'Ġthence', 'Ġ[', 'am', ']', 'Ġi', ':', 'Ġand', 'Ġi', 'Ġwent', 'Ġto', 'Ġbethlehemjudah', ',', 'Ġbut', 'Ġi', 'Ġ[', 'am', 'Ġnow', ']', 'Ġgoing', 'Ġto', 'Ġthe', 'Ġhouse', 'Ġof', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġthere', 'Ġ[', 'is', ']', 'Ġno', 'Ġman', 'Ġthat', 'Ġreceiveth', 'Ġme', 'Ġto', 'Ġhouse', '.']\n",
      "and ye shall appoint the possession of the city five thousand broad, and five and twenty thousand long, over against the oblation of the holy [portion]: it shall be for the whole house of israel.\n",
      "['Ġand', 'Ġye', 'Ġshall', 'Ġappoint', 'Ġthe', 'Ġpossession', 'Ġof', 'Ġthe', 'Ġcity', 'Ġfive', 'Ġthousand', 'Ġbroad', ',', 'Ġand', 'Ġfive', 'Ġand', 'Ġtwenty', 'Ġthousand', 'Ġlong', ',', 'Ġover', 'Ġagainst', 'Ġthe', 'Ġoblation', 'Ġof', 'Ġthe', 'Ġholy', 'Ġ[', 'portion', ']:', 'Ġit', 'Ġshall', 'Ġbe', 'Ġfor', 'Ġthe', 'Ġwhole', 'Ġhouse', 'Ġof', 'Ġisrael', '.']\n",
      "while it is said, to day if ye will hear his voice, harden not your hearts, as in the provocation.\n",
      "['Ġwhile', 'Ġit', 'Ġis', 'Ġsaid', ',', 'Ġto', 'Ġday', 'Ġif', 'Ġye', 'Ġwill', 'Ġhear', 'Ġhis', 'Ġvoice', ',', 'Ġharden', 'Ġnot', 'Ġyour', 'Ġhearts', ',', 'Ġas', 'Ġin', 'Ġthe', 'Ġprovocation', '.']\n",
      "they that swear by the sin of samaria, and say, thy god, o dan, liveth; and, the manner of beersheba liveth; even they shall fall, and never rise up again.\n",
      "['Ġthey', 'Ġthat', 'Ġswear', 'Ġby', 'Ġthe', 'Ġsin', 'Ġof', 'Ġsamaria', ',', 'Ġand', 'Ġsay', ',', 'Ġthy', 'Ġgod', ',', 'Ġo', 'Ġdan', ',', 'Ġliveth', ';', 'Ġand', ',', 'Ġthe', 'Ġmanner', 'Ġof', 'Ġbeersheba', 'Ġliveth', ';', 'Ġeven', 'Ġthey', 'Ġshall', 'Ġfall', ',', 'Ġand', 'Ġnever', 'Ġrise', 'Ġup', 'Ġagain', '.']\n",
      "now a certain man of the servants of saul [was] there that day, detained before the lord; and his name [was] doeg, an edomite, the chiefest of the herdmen that [belonged] to saul.\n",
      "['Ġnow', 'Ġa', 'Ġcertain', 'Ġman', 'Ġof', 'Ġthe', 'Ġservants', 'Ġof', 'Ġsaul', 'Ġ[', 'was', ']', 'Ġthere', 'Ġthat', 'Ġday', ',', 'Ġdetained', 'Ġbefore', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġhis', 'Ġname', 'Ġ[', 'was', ']', 'Ġdoeg', ',', 'Ġan', 'Ġedomite', ',', 'Ġthe', 'Ġchiefest', 'Ġof', 'Ġthe', 'Ġherdmen', 'Ġthat', 'Ġ[', 'belonged', ']', 'Ġto', 'Ġsaul', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(english_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(english_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bpe_gpt2 = AutoTokenizer.from_pretrained('gpt2', padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_gpt2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġshoes', 'Mexico', 'Ġinaction', 'asc', 'Ġshared', 'ĠTrack', 'Ġsubord', 'modern', 'Ġtubes', 'Ïī']\n"
     ]
    }
   ],
   "source": [
    "print(list(bpe_gpt2.vocab)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'Ġfighters', 'Ġin', 'Ġthe', 'ĠUFC', 'Ġwere', 'Ġstriking', 'Ġeach', 'Ġother', ',', 'Ġthen', 'Ġone', 'Ġof', 'Ġthem', 'Ġended', 'Ġup', 'Ġtaking', 'Ġhis', 'Ġopponent', 'Ġdown', 'Ġand', 'Ġsubmitting', 'Ġhim', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = bpe_gpt2.tokenize('Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.')\n",
    "print(tokens)\n",
    "ids = bpe_gpt2.encode('Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.')\n",
    "bpe_gpt2.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bat', 'ting']\n",
      "['dis', 'eng', 'age']\n",
      "['ar', 'che', 'ologists']\n",
      "['ey', 'ewitness', 'es']\n",
      "['phot', 'ographers']\n"
     ]
    }
   ],
   "source": [
    "sample_words = 'batting disengage archeologists eyewitnesses photographers'\n",
    "for word in sample_words.split():\n",
    "    tokens = bpe_gpt2.tokenize(word)\n",
    "    print(tokens)\n",
    "    # ids = bpe_gpt2.encode(word)\n",
    "    # print(bpe_gpt2.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġbatt', 'ing']\n",
      "['Ġdis', 'en', 'g', 'age']\n",
      "['Ġarch', 'eo', 'log', 'ist', 's']\n",
      "['Ġeyewitnesses']\n",
      "['Ġp', 'ho', 'to', 'g', 'raph', 'ers']\n",
      "18627 50257\n"
     ]
    }
   ],
   "source": [
    "sample_words = 'batting disengage archeologists eyewitnesses photographers'\n",
    "for word in sample_words.split():\n",
    "    tokens = eng_tokenizer.encode(word).tokens\n",
    "    print(tokens)\n",
    "print(eng_tokenizer.get_vocab_size(), bpe_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save('data/obolo-bpe-tokenizer.json')\n",
    "# tokenizer = Tokenizer.from_file('data/obolo-bpe-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìkakifieek owot me lek ofiik mgbọ utọọk, mije, ìkilọlọk ofọnti utọọk iso otutuuk ebi uwu kan̄.\n",
      "['Ġi', 'ÌĢ', 'kakifieek', 'Ġowot', 'Ġme', 'Ġlek', 'Ġofiik', 'Ġmgbo', 'Ì£', 'Ġuto', 'Ì£', 'o', 'Ì£', 'k', ',', 'Ġmije', ',', 'Ġi', 'ÌĢ', 'kilo', 'Ì£', 'lo', 'Ì£', 'k', 'Ġofo', 'Ì£', 'nti', 'Ġuto', 'Ì£', 'o', 'Ì£', 'k', 'Ġiso', 'Ġotutuuk', 'Ġebi', 'Ġuwu', 'Ġkan', 'ÌĦ.']\n",
      " ìkakifieek owot me lek ofiik mgbọ utọọk, mije, ìkilọlọk ofọnti utọọk iso otutuuk ebi uwu kan̄.\n"
     ]
    }
   ],
   "source": [
    "print(obolo_data[100])\n",
    "out = tokenizer.encode(obolo_data[100])\n",
    "print(out.tokens)\n",
    "print(tokenizer.decode(out.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=15866, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file='data/obolo-bpe-tokenizer.json')\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56, 55, 3255, 56, 687, 56, 55, 750, 118, 104, 7992, 709, 53, 1158, 53, 16, 56, 95, 425, 10, 156, 526, 50, 152, 191, 56, 55, 470, 53, 108, 87, 109, 177, 49, 81, 587, 149]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = fast_tokenizer([\"mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.\"], add_special_tokens=True)\n",
    "print(ids.get('input_ids'))\n",
    "print(fast_tokenizer.convert_tokens_to_ids('[UNK]'))\n",
    "fast_tokenizer.batch_decode(ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 3,\n",
       " 23,\n",
       " 346,\n",
       " 15719,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 210,\n",
       " 17815,\n",
       " 1889,\n",
       " 3522,\n",
       " 19,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 3,\n",
       " 51,\n",
       " 5115,\n",
       " 3,\n",
       " 40,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 5,\n",
       " 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tk.batch_encode_plus(obolo_data[:5])['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse244",
   "language": "python",
   "name": "mse244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
