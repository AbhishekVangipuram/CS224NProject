{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "# model_id = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# model_id = \"google-t5/t5-base\"\n",
    "\n",
    "# does not download the pretrained weights, just affects configuration\n",
    "# use AutoModelForSeq2SeqLM.from_pretrained to also download the weights\n",
    "# config = AutoConfig.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now, need to add a tokenizer and embedding layer to the top of the model\n",
    "auto_tk = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "['”', '▁punct', '▁taken', 'elecommunication', '▁cables', '▁helpful', 'cita', 'gasesc', '▁buyer', 'aide', '▁bisher', '▁upgrading', '▁Haftung', '▁crunchy', '▁Colegi', '▁Bollywood', '▁historical', '▁NASA', '▁Minute', 'teamed', '▁peace', '▁Diesel', '▁--', 'gate', '▁zip', '▁zuständig', '▁define', '▁Digi', '▁Diversity', '▁engage', '▁peninsula', 'moni', '▁phone', '▁reflecting', '▁experienta', 'blin', '▁poems', 'zugleich', '▁force', 'brücke', 'Lib', 'cul', 'ministerium', '▁song', '▁themes', '▁suis', 'admi', 'gesagt', 'â', 'OC', '▁birouri', '▁activités', '▁franchi', '▁Cushion', '▁Versand', '▁mittels', '▁strig', '▁diffusion', 'lebt', '▁payment', '▁crashes', '▁Qualcomm', '▁Strange', 'lov', '▁nächste', '▁Januar', '▁bestellen', '▁Sat', '▁aplicat', '▁revolution', 'soluble', '▁legend', 'terribly', '▁fitted', '▁run', '▁everyone', '▁ramp', '▁Fotos', 'absorbed', '▁ignor', '▁Bangalore', '▁Commissioner', 'ani', '▁funnel', '▁Proceedings', 'erweise', 'tech', '▁GREAT', '▁dilemma', 'identifying', '▁(\"', 'opia', 'ărilor', '▁ferry', 'transforming', '▁vân', 'Nu', 'obtenir', 'geschrieben', '▁vision']\n"
     ]
    }
   ],
   "source": [
    "print(auto_tk.vocab_size)\n",
    "print(list(auto_tk.get_vocab())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v4.csv\")\n",
    "# data_splits = dataset['train'].train_test_split(0.1)\n",
    "# train, test = data_splits['train'], data_splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./data/train.csv')\n",
    "# test.to_csv('./data/test.csv')\n",
    "train = load_dataset('csv', data_files='data/train.csv')['train']\n",
    "test = load_dataset('csv', data_files='data/test.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7150\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "# try to restrict vocab size to be similar to our custom tokenizer vocab\n",
    "CUSTOM_VOCAB_SIZE = 7150\n",
    "trainer = BpeTrainer(vocab_size=CUSTOM_VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "\n",
    "obolo_data = train['Obolo']\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train_from_iterator(obolo_data, trainer)\n",
    "print(tokenizer.get_vocab_size())\n",
    "tokenizer.save('./custom_tokenizers/obolo-bpe-tokenizer-small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7150\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "# try to restrict vocab size to be similar to our custom tokenizer vocab\n",
    "CUSTOM_VOCAB_SIZE = 7150\n",
    "trainer = WordPieceTrainer(vocab_size=CUSTOM_VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "\n",
    "obolo_data = train['Obolo']\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "tokenizer.train_from_iterator(obolo_data, trainer)\n",
    "print(tokenizer.get_vocab_size())\n",
    "tokenizer.save('./custom_tokenizers/obolo-wordpiece-tokenizer-small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BPE Tokenizer Length: 17596\n",
      "{'input_ids': [5750, 610, 66, 2751, 35, 516, 178, 61, 233], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "7150\n",
      "7302\n",
      "{'input_ids': [6936, 513, 296, 225, 2363, 3933, 347, 358], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [89, 1087, 233, 42, 185, 66, 42, 18623, 42, 9386, 42, 1030], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "8343\n",
      "8343\n",
      "17596\n",
      "18637\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "obolo_tokenizer = PreTrainedTokenizerFast(tokenizer_file='./custom_tokenizers/obolo-bpe-tokenizer.json', padding='left')\n",
    "print(\"Original BPE Tokenizer Length:\", obolo_tokenizer.vocab_size)\n",
    "test_sentence = \"jeremot liyi luku sene kot\"\n",
    "print(obolo_tokenizer(test_sentence))\n",
    "custom_token_set = dict(json.load( open( \"./custom_tokenizers/custom_obolo_tokenizer_vocab_2.json\" ) )).keys()\n",
    "# Train a new tokenizer using the am_train and the old tokenizer object.\n",
    "print(len(custom_token_set))\n",
    "new_tokenizer = obolo_tokenizer.train_new_from_iterator(custom_token_set, vocab_size=100000)\n",
    "print(len(set(new_tokenizer.vocab).intersection(set(obolo_tokenizer.vocab))))\n",
    "obolo_tokenizer.add_tokens(list(new_tokenizer.vocab))\n",
    "\n",
    "print(new_tokenizer(test_sentence))\n",
    "print(obolo_tokenizer(test_sentence))\n",
    "\n",
    "print(new_tokenizer.vocab_size)\n",
    "print(len(new_tokenizer))\n",
    "print(obolo_tokenizer.vocab_size)\n",
    "print(len(obolo_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6864\n"
     ]
    }
   ],
   "source": [
    "tokenizer_small = PreTrainedTokenizerFast(tokenizer_file='./custom_tokenizers/obolo-bpe-tokenizer-small.json', padding='left')\n",
    "print(len(tokenizer_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18887\n"
     ]
    }
   ],
   "source": [
    "# obolo_tokenizer.save_pretrained(save_directory='./custom_tokenizers/custom_plus_bpe_obolo_tokenizer')\n",
    "obolo_tokenizer = PreTrainedTokenizerFast(tokenizer_file='./custom_tokenizers/custom_plus_bpe_obolo_tokenizer/tokenizer.json', padding='left')\n",
    "print(len(obolo_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21988\n"
     ]
    }
   ],
   "source": [
    "# find total words and unique words in the Obolo set\n",
    "import string\n",
    "data = train['Obolo']+test['Obolo']\n",
    "unique_words_verse = [set(s.translate(str.maketrans('', '', string.punctuation)).strip().split()) for s in data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "freq = defaultdict(int)\n",
    "for s in data:\n",
    "    words = s.translate(str.maketrans('', '', string.punctuation)).strip().split()\n",
    "    for w in words:\n",
    "        freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8865"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in freq.items():\n",
    "    if v == 1:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882513"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = sum(freq.values()) \n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17596\n",
      "30600\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in train['Obolo']]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eya orọ, nnenikana isi lek ogwukaan̄-ene awaji, mè ito ito inyi, mè iriaak uriaak, mè ibeek mbeek, mè ijin otu, mè itap ofọnti ufieejit, mè ifuun̄ lek me ntọn̄.\n",
      "['Ġeya', 'Ġorá»į', ',', 'Ġnnenikana', 'Ġisi', 'Ġlek', 'Ġogwukaan', 'ÌĦ-', 'ene', 'Ġawaji', ',', 'Ġme', 'ÌĢ', 'Ġito', 'Ġito', 'Ġinyi', ',', 'Ġme', 'ÌĢ', 'Ġiriaak', 'Ġuriaak', ',', 'Ġme', 'ÌĢ', 'Ġibeek', 'Ġmbeek', ',', 'Ġme', 'ÌĢ', 'Ġijin', 'Ġotu', ',', 'Ġme', 'ÌĢ', 'Ġitap', 'Ġofá»įnti', 'Ġufieejit', ',', 'Ġme', 'ÌĢ', 'Ġifuun', 'ÌĦ', 'Ġlek', 'Ġme', 'Ġntá»įn', 'ÌĦ.']\n",
      "bọn ebirieen̄ kè kora ìre asiri mè elikana mè abiasafu. ebibi ere ebi otoko kè kora me ototun̄ ukan̄ me ototun̄ ukan̄.\n",
      "['Ġbá»įn', 'Ġebirieen', 'ÌĦ', 'Ġke', 'ÌĢ', 'Ġkora', 'Ġi', 'ÌĢ', 're', 'Ġasiri', 'Ġme', 'ÌĢ', 'Ġelikana', 'Ġme', 'ÌĢ', 'Ġabiasafu', '.', 'Ġebibi', 'Ġere', 'Ġebi', 'Ġotoko', 'Ġke', 'ÌĢ', 'Ġkora', 'Ġme', 'Ġototun', 'ÌĦ', 'Ġukan', 'ÌĦ', 'Ġme', 'Ġototun', 'ÌĦ', 'Ġukan', 'ÌĦ.']\n",
      "enyi mêchieen̄ ita me etete ebi ofifi ido. ijọn̄ ebi ochicha kinyi môrie enyi.\n",
      "['Ġenyi', 'Ġme', 'ÌĤ', 'chieen', 'ÌĦ', 'Ġita', 'Ġme', 'Ġetete', 'Ġebi', 'Ġofifi', 'Ġido', '.', 'Ġijá»įn', 'ÌĦ', 'Ġebi', 'Ġochicha', 'Ġkinyi', 'Ġmo', 'ÌĤ', 'rie', 'Ġenyi', '.']\n",
      "ogwu ònyanyan̄a eji, mè igwen eji ibe eji eneluk ugwem òkup me mbuban. igwen yi awaji ogwenbe eji, ìkare mije inu geege eji erọbe. ire, awaji ìgwen eji mije nkeek eyi îkaan̄be me lek kiji, melek atata nnwọn kan̄ eyi înyibe eji inan̄a me lek kè jisọs karais, sabum enirom linyọn̄.\n",
      "['Ġogwu', 'Ġo', 'ÌĢ', 'nyanyan', 'ÌĦ', 'a', 'Ġeji', ',', 'Ġme', 'ÌĢ', 'Ġigwen', 'Ġeji', 'Ġibe', 'Ġeji', 'Ġeneluk', 'Ġugwem', 'Ġo', 'ÌĢ', 'kup', 'Ġme', 'Ġmbuban', '.', 'Ġigwen', 'Ġyi', 'Ġawaji', 'Ġogwenbe', 'Ġeji', ',', 'Ġi', 'ÌĢ', 'kare', 'Ġmije', 'Ġinu', 'Ġgeege', 'Ġeji', 'Ġerá»įbe', '.', 'Ġire', ',', 'Ġawaji', 'Ġi', 'ÌĢ', 'gwen', 'Ġeji', 'Ġmije', 'Ġnkeek', 'Ġeyi', 'Ġi', 'ÌĤ', 'kaan', 'ÌĦ', 'be', 'Ġme', 'Ġlek', 'Ġkiji', ',', 'Ġmelek', 'Ġatata', 'Ġnnwá»įn', 'Ġkan', 'ÌĦ', 'Ġeyi', 'Ġi', 'ÌĤ', 'nyibe', 'Ġeji', 'Ġinan', 'ÌĦ', 'a', 'Ġme', 'Ġlek', 'Ġke', 'ÌĢ', 'Ġjisá»įs', 'Ġkarais', ',', 'Ġsabum', 'Ġenirom', 'Ġlinyá»įn', 'ÌĦ.']\n",
      "ebi filisia erieen̄ akọn̄ me inyọn̄ orioon̄ ge, ebi ijeren ererieen̄ akọn̄ eyi kiban̄ me inyọn̄ ofifi orioon̄ òkibene isi ikpọ eyi ebi filisia ekupbe me inyọn̄. sà iteke ijọn̄ onikup me etete kiban̄.\n",
      "['Ġebi', 'Ġfilisia', 'Ġerieen', 'ÌĦ', 'Ġaká»įn', 'ÌĦ', 'Ġme', 'Ġinyá»įn', 'ÌĦ', 'Ġorioon', 'ÌĦ', 'Ġge', ',', 'Ġebi', 'Ġijeren', 'Ġererieen', 'ÌĦ', 'Ġaká»įn', 'ÌĦ', 'Ġeyi', 'Ġkiban', 'ÌĦ', 'Ġme', 'Ġinyá»įn', 'ÌĦ', 'Ġofifi', 'Ġorioon', 'ÌĦ', 'Ġo', 'ÌĢ', 'kibene', 'Ġisi', 'Ġikpá»į', 'Ġeyi', 'Ġebi', 'Ġfilisia', 'Ġekupbe', 'Ġme', 'Ġinyá»įn', 'ÌĦ.', 'Ġsa', 'ÌĢ', 'Ġiteke', 'Ġijá»įn', 'ÌĦ', 'Ġonikup', 'Ġme', 'Ġetete', 'Ġkiban', 'ÌĦ.']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(obolo_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(obolo_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "english_data = train['English']\n",
    "\n",
    "eng_tokenizer.pre_tokenizer = ByteLevel()\n",
    "eng_tokenizer.decoder = decoders.ByteLevel()\n",
    "eng_tokenizer.train_from_iterator(english_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18649\n",
      "28148\n"
     ]
    }
   ],
   "source": [
    "print(eng_tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in english_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and he said unto him, we [are] passing from bethlehemjudah toward the side of mount ephraim; from thence [am] i: and i went to bethlehemjudah, but i [am now] going to the house of the lord; and there [is] no man that receiveth me to house.\n",
      "['Ġand', 'Ġhe', 'Ġsaid', 'Ġunto', 'Ġhim', ',', 'Ġwe', 'Ġ[', 'are', ']', 'Ġpassing', 'Ġfrom', 'Ġbethlehemjudah', 'Ġtoward', 'Ġthe', 'Ġside', 'Ġof', 'Ġmount', 'Ġephraim', ';', 'Ġfrom', 'Ġthence', 'Ġ[', 'am', ']', 'Ġi', ':', 'Ġand', 'Ġi', 'Ġwent', 'Ġto', 'Ġbethlehemjudah', ',', 'Ġbut', 'Ġi', 'Ġ[', 'am', 'Ġnow', ']', 'Ġgoing', 'Ġto', 'Ġthe', 'Ġhouse', 'Ġof', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġthere', 'Ġ[', 'is', ']', 'Ġno', 'Ġman', 'Ġthat', 'Ġreceiveth', 'Ġme', 'Ġto', 'Ġhouse', '.']\n",
      "and ye shall appoint the possession of the city five thousand broad, and five and twenty thousand long, over against the oblation of the holy [portion]: it shall be for the whole house of israel.\n",
      "['Ġand', 'Ġye', 'Ġshall', 'Ġappoint', 'Ġthe', 'Ġpossession', 'Ġof', 'Ġthe', 'Ġcity', 'Ġfive', 'Ġthousand', 'Ġbroad', ',', 'Ġand', 'Ġfive', 'Ġand', 'Ġtwenty', 'Ġthousand', 'Ġlong', ',', 'Ġover', 'Ġagainst', 'Ġthe', 'Ġoblation', 'Ġof', 'Ġthe', 'Ġholy', 'Ġ[', 'portion', ']:', 'Ġit', 'Ġshall', 'Ġbe', 'Ġfor', 'Ġthe', 'Ġwhole', 'Ġhouse', 'Ġof', 'Ġisrael', '.']\n",
      "while it is said, to day if ye will hear his voice, harden not your hearts, as in the provocation.\n",
      "['Ġwhile', 'Ġit', 'Ġis', 'Ġsaid', ',', 'Ġto', 'Ġday', 'Ġif', 'Ġye', 'Ġwill', 'Ġhear', 'Ġhis', 'Ġvoice', ',', 'Ġharden', 'Ġnot', 'Ġyour', 'Ġhearts', ',', 'Ġas', 'Ġin', 'Ġthe', 'Ġprovocation', '.']\n",
      "they that swear by the sin of samaria, and say, thy god, o dan, liveth; and, the manner of beersheba liveth; even they shall fall, and never rise up again.\n",
      "['Ġthey', 'Ġthat', 'Ġswear', 'Ġby', 'Ġthe', 'Ġsin', 'Ġof', 'Ġsamaria', ',', 'Ġand', 'Ġsay', ',', 'Ġthy', 'Ġgod', ',', 'Ġo', 'Ġdan', ',', 'Ġliveth', ';', 'Ġand', ',', 'Ġthe', 'Ġmanner', 'Ġof', 'Ġbeersheba', 'Ġliveth', ';', 'Ġeven', 'Ġthey', 'Ġshall', 'Ġfall', ',', 'Ġand', 'Ġnever', 'Ġrise', 'Ġup', 'Ġagain', '.']\n",
      "now a certain man of the servants of saul [was] there that day, detained before the lord; and his name [was] doeg, an edomite, the chiefest of the herdmen that [belonged] to saul.\n",
      "['Ġnow', 'Ġa', 'Ġcertain', 'Ġman', 'Ġof', 'Ġthe', 'Ġservants', 'Ġof', 'Ġsaul', 'Ġ[', 'was', ']', 'Ġthere', 'Ġthat', 'Ġday', ',', 'Ġdetained', 'Ġbefore', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġhis', 'Ġname', 'Ġ[', 'was', ']', 'Ġdoeg', ',', 'Ġan', 'Ġedomite', ',', 'Ġthe', 'Ġchiefest', 'Ġof', 'Ġthe', 'Ġherdmen', 'Ġthat', 'Ġ[', 'belonged', ']', 'Ġto', 'Ġsaul', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(english_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(english_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "Custom tokenizer vocab size on train data: 6859\n"
     ]
    }
   ],
   "source": [
    "from custom_tokenizers.tokenizerv1 import parse_sentence\n",
    "tokens = set()\n",
    "for idx, s in enumerate(train['Obolo']):\n",
    "    if idx % 10000 == 0:\n",
    "        print(idx)\n",
    "    tokens.update(set(parse_sentence(s)))\n",
    "print('Custom tokenizer vocab size on train data:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mônikpọ', 'baraken', 'môkilibi', 'nikanọ', 'ǹnenkwala', 'ri', 'magidala', 'mumumuk', 'pirisila', 'mêkiyiyeek']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokens)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mè initumu inyi samien ibe, “kpọ, mgbọ keyi owu ora ikan. nsabọn ebirieen̄ kwun̄ si kpekup kubọk owu okupbe. eya orọ, to ubọọn̄ nyi eji, ogwu òbokikpulu eji, mè ikisibi oyerebet inyi eji kubọk îkupbe me otutuuk ofifi ido geelek.”\n",
      "['mè', 'i', 'ni', 'tumu', 'i', 'nyi', 'samien', 'i', 'be', ',', '“', 'kpọ', ',', 'm', 'gbọ', 'keyi', 'o', 'wu', 'o', 'ra', 'i', 'kan', '.', 'nsabọn', 'ebirieen̄', 'kwun̄', 'si', 'kpe', 'kup', 'kubọk', 'o', 'wu', 'o', 'kup', 'be', '.', 'e', 'ya', 'o', 'rọ', ',', 'to', 'ubọọn̄', 'n', 'yi', 'e', 'ji', ',', 'o', 'gwu', 'o', '̀', 'bo', 'ki', 'kpulu', 'e', 'ji', ',', 'mè', 'i', 'ki', 'sibi', 'o', 'yerebet', 'i', 'nyi', 'e', 'ji', 'kubọk', 'i', '̂', 'kup', 'be', 'me', 'o', 'REDUP', 'tuuk', 'o', 'REDUP', 'fi', 'i', 'do', 'geelek', '.', '”']\n",
      "mèinitumuinyisamienibe,“kpọ,mgbọkeyiowuoraikan.nsabọnebirieen̄kwun̄sikpekupkubọkowuokupbe.eyaorọ,toubọọn̄nyieji,ogwuòbokikpulueji,mèikisibioyerebetinyiejikubọkîkupbemeotutuukofifiidogeelek.”\n",
      "mèinitumuinyisamienibe,“kpọ,mgbọkeyiowuoraikan.nsabọnebirieen̄kwun̄sikpekupkubọkowuokupbe.eyaorọ,toubọọn̄nyieji,ogwuòbokikpulueji,mèikisibioyerebetinyiejikubọkîkupbemeoREDUPtuukoREDUPfiidogeelek.”\n"
     ]
    }
   ],
   "source": [
    "sent = test['Obolo'][10]\n",
    "print(sent)\n",
    "print(parse_sentence(sent))\n",
    "print(sent.replace(' ', ''))\n",
    "print(''.join(parse_sentence(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27987 3110 31097\n"
     ]
    }
   ],
   "source": [
    "print(len(train['Obolo']), len(test['Obolo']), len(train['Obolo'])+len(test['Obolo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bpe_gpt2 = AutoTokenizer.from_pretrained('gpt2', padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_gpt2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġshoes', 'Mexico', 'Ġinaction', 'asc', 'Ġshared', 'ĠTrack', 'Ġsubord', 'modern', 'Ġtubes', 'Ïī']\n"
     ]
    }
   ],
   "source": [
    "print(list(bpe_gpt2.vocab)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'Ġfighters', 'Ġin', 'Ġthe', 'ĠUFC', 'Ġwere', 'Ġstriking', 'Ġeach', 'Ġother', ',', 'Ġthen', 'Ġone', 'Ġof', 'Ġthem', 'Ġended', 'Ġup', 'Ġtaking', 'Ġhis', 'Ġopponent', 'Ġdown', 'Ġand', 'Ġsubmitting', 'Ġhim', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = bpe_gpt2.tokenize('Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.')\n",
    "print(tokens)\n",
    "ids = bpe_gpt2.encode('Two fighters in the UFC were striking each other, then one of them ended up taking his opponent down and submitting him.')\n",
    "bpe_gpt2.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bat', 'ting']\n",
      "['dis', 'eng', 'age']\n",
      "['ar', 'che', 'ologists']\n",
      "['ey', 'ewitness', 'es']\n",
      "['phot', 'ographers']\n"
     ]
    }
   ],
   "source": [
    "sample_words = 'batting disengage archeologists eyewitnesses photographers'\n",
    "for word in sample_words.split():\n",
    "    tokens = bpe_gpt2.tokenize(word)\n",
    "    print(tokens)\n",
    "    # ids = bpe_gpt2.encode(word)\n",
    "    # print(bpe_gpt2.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġbatt', 'ing']\n",
      "['Ġdis', 'en', 'g', 'age']\n",
      "['Ġarch', 'eo', 'log', 'ist', 's']\n",
      "['Ġeyewitnesses']\n",
      "['Ġp', 'ho', 'to', 'g', 'raph', 'ers']\n",
      "18627 50257\n"
     ]
    }
   ],
   "source": [
    "sample_words = 'batting disengage archeologists eyewitnesses photographers'\n",
    "for word in sample_words.split():\n",
    "    tokens = eng_tokenizer.encode(word).tokens\n",
    "    print(tokens)\n",
    "print(eng_tokenizer.get_vocab_size(), bpe_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save('data/obolo-bpe-tokenizer.json')\n",
    "# tokenizer = Tokenizer.from_file('data/obolo-bpe-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìkakifieek owot me lek ofiik mgbọ utọọk, mije, ìkilọlọk ofọnti utọọk iso otutuuk ebi uwu kan̄.\n",
      "['Ġi', 'ÌĢ', 'kakifieek', 'Ġowot', 'Ġme', 'Ġlek', 'Ġofiik', 'Ġmgbo', 'Ì£', 'Ġuto', 'Ì£', 'o', 'Ì£', 'k', ',', 'Ġmije', ',', 'Ġi', 'ÌĢ', 'kilo', 'Ì£', 'lo', 'Ì£', 'k', 'Ġofo', 'Ì£', 'nti', 'Ġuto', 'Ì£', 'o', 'Ì£', 'k', 'Ġiso', 'Ġotutuuk', 'Ġebi', 'Ġuwu', 'Ġkan', 'ÌĦ.']\n",
      " ìkakifieek owot me lek ofiik mgbọ utọọk, mije, ìkilọlọk ofọnti utọọk iso otutuuk ebi uwu kan̄.\n"
     ]
    }
   ],
   "source": [
    "print(obolo_data[100])\n",
    "out = tokenizer.encode(obolo_data[100])\n",
    "print(out.tokens)\n",
    "print(tokenizer.decode(out.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=15866, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file='data/obolo-bpe-tokenizer.json')\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56, 55, 3255, 56, 687, 56, 55, 750, 118, 104, 7992, 709, 53, 1158, 53, 16, 56, 95, 425, 10, 156, 526, 50, 152, 191, 56, 55, 470, 53, 108, 87, 109, 177, 49, 81, 587, 149]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = fast_tokenizer([\"mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.\"], add_special_tokens=True)\n",
    "print(ids.get('input_ids'))\n",
    "print(fast_tokenizer.convert_tokens_to_ids('[UNK]'))\n",
    "fast_tokenizer.batch_decode(ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 3,\n",
       " 23,\n",
       " 346,\n",
       " 15719,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 210,\n",
       " 17815,\n",
       " 1889,\n",
       " 3522,\n",
       " 19,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 3,\n",
       " 51,\n",
       " 5115,\n",
       " 3,\n",
       " 40,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 5,\n",
       " 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tk.batch_encode_plus(obolo_data[:5])['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
