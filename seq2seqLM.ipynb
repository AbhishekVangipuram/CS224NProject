{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "# model_id = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model_id = \"google-t5/t5-base\"\n",
    "\n",
    "# does not download the pretrained weights, just affects configuration\n",
    "# use AutoModelForSeq2SeqLM.from_pretrained to also download the weights\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef875247ac4f7e87bb6e7f8e380a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhiv\\.cache\\huggingface\\hub\\models--google-t5--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce840fc4381643228ded8dd38f5bf539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now, need to add a tokenizer and embedding layer to the top of the model\n",
    "auto_tk = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "obolo_data = dataset['train']['Obolo']\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train_from_iterator(obolo_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16463\n",
      "32083\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in obolo_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġme', 'Ġibebene', ',', 'Ġawaji', 'Ġi', 'ÌĤ', 'rom', 'Ġisinyo', 'Ì£', 'n', 'ÌĦ', 'Ġme', 'ÌĢ', 'Ġlinyo', 'Ì£', 'n', 'ÌĦ.']\n",
      "['Ġire', ',', 'Ġlinyo', 'Ì£', 'n', 'ÌĦ', 'Ġya', 'Ġi', 'ÌĢ', 'kabet', 'Ġlek', 'Ġinu', 'Ġgeege', ',', 'Ġi', 'ÌĢ', 'kpo', 'Ì£', 'kpo', 'Ì£', 'Ġilaak', 'Ġukpo', 'Ì£', 'k', '.', 'Ġmun', 'ÌĦ', 'Ġmun', 'ÌĦ', 'Ġojot', 'Ġlinyo', 'Ì£', 'n', 'ÌĦ.', 'Ġudun', 'Ġokakana', 'Ġotutuuk', 'Ġere', 'Ġichit', 'Ġbu', 'ÌĢ', 'k', '.', 'Ġekwukwu', 'Ġawaji', 'Ġokekiwulu', 'Ġefe', 'Ġigba', 'Ġiyak', 'Ġikana', 'Ġme', 'Ġinyo', 'Ì£', 'n', 'ÌĦ', 'Ġisi', 'Ġmun', 'ÌĦ.']\n",
      "['Ġawaji', 'Ġonenitumu', 'Ġibe', ',', 'ĠâĢľ', 'utoon', 'ÌĦ', 'Ġikup', '!âĢĿ', 'Ġutoon', 'ÌĦ', 'Ġokukup', '.']\n",
      "['Ġawaji', 'Ġomumun', 'ÌĦ', 'Ġibe', 'Ġke', 'Ġutoon', 'ÌĦ', 'Ġya', 'Ġi', 'ÌĤ', 'jaan', 'ÌĦ,', 'Ġme', 'ÌĢ', 'Ġiniche', 'Ġutoon', 'ÌĦ', 'Ġya', 'Ġisan', 'ÌĦ', 'a', 'Ġme', 'Ġlek', 'Ġudun', 'Ġya', ',']\n",
      "['Ġme', 'ÌĢ', 'Ġigwen', 'Ġutoon', 'ÌĦ', 'Ġya', 'Ġegwe', ',', 'Ġme', 'ÌĢ', 'Ġinigwen', 'Ġudun', 'Ġya', 'Ġeririeen', 'ÌĦ.', 'Ġeririeen', 'ÌĦ', 'Ġokukup', ',', 'Ġegwe', 'Ġokukup', '.', 'Ġeya', 'Ġorere', 'Ġadasi', 'Ġusen', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(obolo_data[:5])\n",
    "for enc in encoded_inputs:\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/obolo-bpe-tokenizer.json')\n",
    "# tokenizer = Tokenizer.from_file('data/obolo-bpe-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġjaban', 'Ġonenikaan', 'ÌĦ', 'Ġngwan', 'ÌĦ', 'Ġenerieen', 'ÌĦ', 'Ġge', 'Ġekigwen', 'Ġjuban', '.', 'Ġjuban', 'Ġore', 'Ġadasi', 'Ġene', 'Ġo', 'ÌĢ', 'bebene', 'Ġikikwak', 'Ġuneen', 'ÌĦ,', 'Ġme', 'ÌĢ', 'Ġikiwut', 'Ġo', 'ÌĢ', 'ja', '.']\n",
      " jaban onenikaan̄ ngwan̄ enerieen̄ ge ekigwen juban. juban ore adasi ene òbebene ikikwak uneen̄, mè ikiwut òja.\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(obolo_data[100])\n",
    "print(out.tokens)\n",
    "print(tokenizer.decode(out.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=16463, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file='data/obolo-bpe-tokenizer.json')\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = fast_tokenizer.encode(\"mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.\")\n",
    "fast_tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 3,\n",
       " 23,\n",
       " 346,\n",
       " 15719,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 210,\n",
       " 17815,\n",
       " 1889,\n",
       " 3522,\n",
       " 19,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 3,\n",
       " 51,\n",
       " 5115,\n",
       " 3,\n",
       " 40,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 5,\n",
       " 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tk.batch_encode_plus(obolo_data[:5])['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse244",
   "language": "python",
   "name": "mse244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
