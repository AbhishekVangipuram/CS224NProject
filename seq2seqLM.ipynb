{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "# model_id = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model_id = \"google-t5/t5-base\"\n",
    "\n",
    "# does not download the pretrained weights, just affects configuration\n",
    "# use AutoModelForSeq2SeqLM.from_pretrained to also download the weights\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhiv\\miniconda3\\envs\\mse244\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now, need to add a tokenizer and embedding layer to the top of the model\n",
    "auto_tk = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32100\n",
      "['”', '▁punct', '▁taken', 'elecommunication', '▁cables', '▁helpful', 'cita', 'gasesc', '▁buyer', 'aide', '▁bisher', '▁upgrading', '▁Haftung', '▁crunchy', '▁Colegi', '▁Bollywood', '▁historical', '▁NASA', '▁Minute', 'teamed', '▁peace', '▁Diesel', '▁--', 'gate', '▁zip', '▁zuständig', '▁define', '▁Digi', '▁Diversity', '▁engage', '▁peninsula', 'moni', '▁phone', '▁reflecting', '▁experienta', 'blin', '▁poems', 'zugleich', '▁force', 'brücke', 'Lib', 'cul', 'ministerium', '▁song', '▁themes', '▁suis', 'admi', 'gesagt', 'â', 'OC', '▁birouri', '▁activités', '▁franchi', '▁Cushion', '▁Versand', '▁mittels', '▁strig', '▁diffusion', 'lebt', '▁payment', '▁crashes', '▁Qualcomm', '▁Strange', 'lov', '▁nächste', '▁Januar', '▁bestellen', '▁Sat', '▁aplicat', '▁revolution', 'soluble', '▁legend', 'terribly', '▁fitted', '▁run', '▁everyone', '▁ramp', '▁Fotos', 'absorbed', '▁ignor', '▁Bangalore', '▁Commissioner', 'ani', '▁funnel', '▁Proceedings', 'erweise', 'tech', '▁GREAT', '▁dilemma', 'identifying', '▁(\"', 'opia', 'ărilor', '▁ferry', 'transforming', '▁vân', 'Nu', 'obtenir', 'geschrieben', '▁vision']\n"
     ]
    }
   ],
   "source": [
    "print(auto_tk.vocab_size)\n",
    "print(list(auto_tk.get_vocab())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"data/v3.csv\")\n",
    "# data_splits = dataset['train'].train_test_split(0.1)\n",
    "# train, test = data_splits['train'], data_splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('./data/train.csv')\n",
    "# test.to_csv('./data/test.csv')\n",
    "train = load_dataset('csv', data_files='data/train.csv')['train']\n",
    "test = load_dataset('csv', data_files='data/test.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "\n",
    "\n",
    "obolo_data = train['Obolo']\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train_from_iterator(obolo_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15852\n",
      "30505\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in obolo_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jisọs oneninan̄a me agan̄ galili isi agba okwaan̄ jodan, inyi jọn igwook ọmọ mun̄.\n",
      "['Ġjiso', 'Ì£', 's', 'Ġoneninan', 'ÌĦ', 'a', 'Ġme', 'Ġagan', 'ÌĦ', 'Ġgalili', 'Ġisi', 'Ġagba', 'Ġokwaan', 'ÌĦ', 'Ġjodan', ',', 'Ġinyi', 'Ġjo', 'Ì£', 'n', 'Ġigwook', 'Ġo', 'Ì£', 'mo', 'Ì£', 'Ġmun', 'ÌĦ.']\n",
      "eyiyi ore adasi ikan, mè ire si eyi òmimin ichit.\n",
      "['Ġeyiyi', 'Ġore', 'Ġadasi', 'Ġikan', ',', 'Ġme', 'ÌĢ', 'Ġire', 'Ġsi', 'Ġeyi', 'Ġo', 'ÌĢ', 'mimin', 'Ġichit', '.']\n",
      "mije ema kpetet inyan̄a-ibot awaji onyan̄abe melek kiban̄ inye; kpesusun̄ si ikan kan̄.\n",
      "['Ġmije', 'Ġema', 'Ġkpetet', 'Ġinyan', 'ÌĦ', 'a', '-', 'ibot', 'Ġawaji', 'Ġonyan', 'ÌĦ', 'abe', 'Ġmelek', 'Ġkiban', 'ÌĦ', 'Ġinye', ';', 'Ġkpesusun', 'ÌĦ', 'Ġsi', 'Ġikan', 'Ġkan', 'ÌĦ.']\n",
      "ebilene mêlilim inin̄ emen ǹkororok lek ewuuk, mè emen udọn̄ me inyọn̄ ijọn̄, inyi ema enan̄a me lek ikpele ukpook okumugwem, mè òrirọ eyi umin ubọọn̄ kan̄, me mgbọ okumugwem môjibibe isibi inu inisisik linyọn̄.\n",
      "['Ġebilene', 'Ġme', 'ÌĤ', 'lilim', 'Ġinin', 'ÌĦ', 'Ġemen', 'Ġn', 'ÌĢ', 'kororok', 'Ġlek', 'Ġewuuk', ',', 'Ġme', 'ÌĢ', 'Ġemen', 'Ġudo', 'Ì£', 'n', 'ÌĦ', 'Ġme', 'Ġinyo', 'Ì£', 'n', 'ÌĦ', 'Ġijo', 'Ì£', 'n', 'ÌĦ,', 'Ġinyi', 'Ġema', 'Ġenan', 'ÌĦ', 'a', 'Ġme', 'Ġlek', 'Ġikpele', 'Ġukpook', 'Ġokumugwem', ',', 'Ġme', 'ÌĢ', 'Ġo', 'ÌĢ', 'riro', 'Ì£', 'Ġeyi', 'Ġumin', 'Ġubo', 'Ì£', 'o', 'Ì£', 'n', 'ÌĦ', 'Ġkan', 'ÌĦ,', 'Ġme', 'Ġmgbo', 'Ì£', 'Ġokumugwem', 'Ġmo', 'ÌĤ', 'jibibe', 'Ġisibi', 'Ġinu', 'Ġinisisik', 'Ġlinyo', 'Ì£', 'n', 'ÌĦ.']\n",
      "nomi otutumu inyi rufu nwa ogwu gwun̄ ibe, “gwun̄ n̄a, înwọn ibe owu òkogọọk ebi ikwaan̄ kan̄ ebibaan̄ òkosibi òsi uko kiban̄. mije, ire osi inin̄ uko ebi ofifi ife, owu môkọt iben unan.”\n",
      "['Ġnomi', 'Ġotutumu', 'Ġinyi', 'Ġrufu', 'Ġnwa', 'Ġogwu', 'Ġgwun', 'ÌĦ', 'Ġibe', ',', 'ĠâĢľ', 'gwun', 'ÌĦ', 'Ġn', 'ÌĦ', 'a', ',', 'Ġi', 'ÌĤ', 'nwo', 'Ì£', 'n', 'Ġibe', 'Ġowu', 'Ġo', 'ÌĢ', 'kogo', 'Ì£', 'o', 'Ì£', 'k', 'Ġebi', 'Ġikwaan', 'ÌĦ', 'Ġkan', 'ÌĦ', 'Ġebibaan', 'ÌĦ', 'Ġo', 'ÌĢ', 'kosibi', 'Ġo', 'ÌĢ', 'si', 'Ġuko', 'Ġkiban', 'ÌĦ.', 'Ġmije', ',', 'Ġire', 'Ġosi', 'Ġinin', 'ÌĦ', 'Ġuko', 'Ġebi', 'Ġofifi', 'Ġife', ',', 'Ġowu', 'Ġmo', 'ÌĤ', 'ko', 'Ì£', 't', 'Ġiben', 'Ġunan', '.âĢĿ']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(obolo_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(obolo_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "english_data = train['English']\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train_from_iterator(english_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15852\n",
      "28190\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.get_vocab_size())\n",
    "unique_words_verse = [set(verse.split()) for verse in english_data]\n",
    "unique_words = set()\n",
    "for s in unique_words_verse:\n",
    "    unique_words.update(s)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and he said unto him, we [are] passing from bethlehemjudah toward the side of mount ephraim; from thence [am] i: and i went to bethlehemjudah, but i [am now] going to the house of the lord; and there [is] no man that receiveth me to house.\n",
      "['Ġand', 'Ġhe', 'Ġsaid', 'Ġunto', 'Ġhim', ',', 'Ġwe', 'Ġ[', 'are', ']', 'Ġpassing', 'Ġfrom', 'Ġbethlehemjudah', 'Ġtoward', 'Ġthe', 'Ġside', 'Ġof', 'Ġmount', 'Ġephraim', ';', 'Ġfrom', 'Ġthence', 'Ġ[', 'am', ']', 'Ġi', ':', 'Ġand', 'Ġi', 'Ġwent', 'Ġto', 'Ġbethlehemjudah', ',', 'Ġbut', 'Ġi', 'Ġ[', 'am', 'Ġnow', ']', 'Ġgoing', 'Ġto', 'Ġthe', 'Ġhouse', 'Ġof', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġthere', 'Ġ[', 'is', ']', 'Ġno', 'Ġman', 'Ġthat', 'Ġreceiveth', 'Ġme', 'Ġto', 'Ġhouse', '.']\n",
      "and ye shall appoint the possession of the city five thousand broad, and five and twenty thousand long, over against the oblation of the holy [portion]: it shall be for the whole house of israel.\n",
      "['Ġand', 'Ġye', 'Ġshall', 'Ġappoint', 'Ġthe', 'Ġpossession', 'Ġof', 'Ġthe', 'Ġcity', 'Ġfive', 'Ġthousand', 'Ġbroad', ',', 'Ġand', 'Ġfive', 'Ġand', 'Ġtwenty', 'Ġthousand', 'Ġlong', ',', 'Ġover', 'Ġagainst', 'Ġthe', 'Ġoblation', 'Ġof', 'Ġthe', 'Ġholy', 'Ġ[', 'portion', ']:', 'Ġit', 'Ġshall', 'Ġbe', 'Ġfor', 'Ġthe', 'Ġwhole', 'Ġhouse', 'Ġof', 'Ġisrael', '.']\n",
      "while it is said, to day if ye will hear his voice, harden not your hearts, as in the provocation.\n",
      "['Ġwhile', 'Ġit', 'Ġis', 'Ġsaid', ',', 'Ġto', 'Ġday', 'Ġif', 'Ġye', 'Ġwill', 'Ġhear', 'Ġhis', 'Ġvoice', ',', 'Ġharden', 'Ġnot', 'Ġyour', 'Ġhearts', ',', 'Ġas', 'Ġin', 'Ġthe', 'Ġprovocation', '.']\n",
      "they that swear by the sin of samaria, and say, thy god, o dan, liveth; and, the manner of beersheba liveth; even they shall fall, and never rise up again.\n",
      "['Ġthey', 'Ġthat', 'Ġswear', 'Ġby', 'Ġthe', 'Ġsin', 'Ġof', 'Ġsamaria', ',', 'Ġand', 'Ġsay', ',', 'Ġthy', 'Ġgod', ',', 'Ġo', 'Ġdan', ',', 'Ġliveth', ';', 'Ġand', ',', 'Ġthe', 'Ġmanner', 'Ġof', 'Ġbeersheba', 'Ġliveth', ';', 'Ġeven', 'Ġthey', 'Ġshall', 'Ġfall', ',', 'Ġand', 'Ġnever', 'Ġrise', 'Ġup', 'Ġagain', '.']\n",
      "now a certain man of the servants of saul [was] there that day, detained before the lord; and his name [was] doeg, an edomite, the chiefest of the herdmen that [belonged] to saul.\n",
      "['Ġnow', 'Ġa', 'Ġcertain', 'Ġman', 'Ġof', 'Ġthe', 'Ġservants', 'Ġof', 'Ġsaul', 'Ġ[', 'was', ']', 'Ġthere', 'Ġthat', 'Ġday', ',', 'Ġdetained', 'Ġbefore', 'Ġthe', 'Ġlord', ';', 'Ġand', 'Ġhis', 'Ġname', 'Ġ[', 'was', ']', 'Ġdoeg', ',', 'Ġan', 'Ġedomite', ',', 'Ġthe', 'Ġchiefest', 'Ġof', 'Ġthe', 'Ġherdmen', 'Ġthat', 'Ġ[', 'belonged', ']', 'Ġto', 'Ġsaul', '.']\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.encode_batch(english_data[:5])\n",
    "for idx, enc in enumerate(encoded_inputs):\n",
    "    print(english_data[idx])\n",
    "    print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bpe_gpt2 = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_gpt2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġrumours', 'Ġaudi', 'Ġunregulated', 'Ġ19', 'Ġhistorian', 'essa', 'Ġrace', 'ĠGorsuch', 'ĠHeaven', 'real']\n"
     ]
    }
   ],
   "source": [
    "print(list(bpe_gpt2.vocab)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('data/obolo-bpe-tokenizer.json')\n",
    "# tokenizer = Tokenizer.from_file('data/obolo-bpe-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġjaban', 'Ġonenikaan', 'ÌĦ', 'Ġngwan', 'ÌĦ', 'Ġenerieen', 'ÌĦ', 'Ġge', 'Ġekigwen', 'Ġjuban', '.', 'Ġjuban', 'Ġore', 'Ġadasi', 'Ġene', 'Ġo', 'ÌĢ', 'bebene', 'Ġikikwak', 'Ġuneen', 'ÌĦ,', 'Ġme', 'ÌĢ', 'Ġikiwut', 'Ġo', 'ÌĢ', 'ja', '.']\n",
      " jaban onenikaan̄ ngwan̄ enerieen̄ ge ekigwen juban. juban ore adasi ene òbebene ikikwak uneen̄, mè ikiwut òja.\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(obolo_data[100])\n",
    "print(out.tokens)\n",
    "print(tokenizer.decode(out.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=16463, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file='data/obolo-bpe-tokenizer.json')\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = fast_tokenizer.encode(\"mè ekekpulu me egwe mè eririeen̄, inyi ekeche utoon̄ esan̄a me lek udun. awaji okpọkpọ, mè imun̄ ibe ke inu cha îjaan̄.\")\n",
    "fast_tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140,\n",
       " 3,\n",
       " 23,\n",
       " 346,\n",
       " 15719,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 210,\n",
       " 17815,\n",
       " 1889,\n",
       " 3522,\n",
       " 19,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 3,\n",
       " 51,\n",
       " 5115,\n",
       " 3,\n",
       " 40,\n",
       " 77,\n",
       " 63,\n",
       " 2,\n",
       " 29,\n",
       " 2,\n",
       " 5,\n",
       " 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tk.batch_encode_plus(obolo_data[:5])['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mse244",
   "language": "python",
   "name": "mse244"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
